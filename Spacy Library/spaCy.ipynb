{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b2ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346a8eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('''\"Let's go to N.Y.!\"''')\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ac1bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      "as\n",
      "it\n",
      "costs\n",
      "only\n",
      "2\n",
      "$\n",
      "per\n",
      "plate\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai as it costs only 2$ per plate.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d32c7a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\", Let, \", Let's go to)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0], doc[1], doc[-1], doc[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3660723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "131c7393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c84262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tony gave two $ to Peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f503bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0 = doc[0]\n",
    "token0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "749a14d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.token.Token"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(token0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59d6558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.is_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf50d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token0.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bc59aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "two"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = doc[2]\n",
    "token2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63c3e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5ddff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3 = doc[3]\n",
    "token3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a300b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token3.is_currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d302c2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony ==> index:  0 is_alpha:  True is_punct:  False like_num:  False is_currency:  False\n",
      "gave ==> index:  1 is_alpha:  True is_punct:  False like_num:  False is_currency:  False\n",
      "two ==> index:  2 is_alpha:  True is_punct:  False like_num:  True is_currency:  False\n",
      "$ ==> index:  3 is_alpha:  False is_punct:  False like_num:  False is_currency:  True\n",
      "to ==> index:  4 is_alpha:  True is_punct:  False like_num:  False is_currency:  False\n",
      "Peter ==> index:  5 is_alpha:  True is_punct:  False like_num:  False is_currency:  False\n",
      ". ==> index:  6 is_alpha:  False is_punct:  True like_num:  False is_currency:  False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, \"==>\", \"index: \", token.i, \n",
    "         \"is_alpha: \", token.is_alpha,\n",
    "         \"is_punct: \", token.is_punct,\n",
    "         \"like_num: \", token.like_num,\n",
    "         \"is_currency: \", token.is_currency,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b5ba6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dayton high school, 8th grade students information\\n',\n",
       " '==================================================\\n',\n",
       " '\\n',\n",
       " 'Name    birth day       email\\n',\n",
       " '----    --------------  -------\\n',\n",
       " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
       " 'Maria   12 April, 2001  maria@sharapova.com\\n',\n",
       " 'Serena  24 June, 1998   serena@williams.com\\n',\n",
       " 'Joe     01 May, 1997    joe@root.com\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"students.txt\") as f:\n",
    "    text = f.readlines()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92d6e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name    birth day       email\\n ----    --------------  -------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria   12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com\\n Joe     01 May, 1997    joe@root.com\\n \\n \\n \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dceca442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virat@kohli.com',\n",
       " 'maria@sharapova.com',\n",
       " 'serena@williams.com',\n",
       " 'joe@root.com']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc =nlp(text)\n",
    "emails = []\n",
    "for token in doc:\n",
    "    if token.like_email:\n",
    "        emails.append(token.text)\n",
    "        \n",
    "emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012de799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "भैया False False\n",
      "जी False False\n",
      "! False False\n",
      "5000 False True\n",
      "₹ True False\n",
      "उधार False False\n",
      "थे False False\n",
      "वो False False\n",
      "वापस False False\n",
      "देदो False False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"hi\")\n",
    "\n",
    "doc = nlp(\"भैया जी! 5000 ₹ उधार थे वो वापस देदो\")\n",
    "for token in doc:\n",
    "    print(token, token.is_currency, token.like_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1abde193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd5e7731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\", [\n",
    "    {ORTH: \"gim\"},\n",
    "    {ORTH: \"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3473d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x292d7d7b280>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6fdeae03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentencizer']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e6865851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange loves pav bhaji of mumbai.\n",
      "Hulk loves chat of delhi.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi.\")\n",
    "\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c3b673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain\n",
      "america\n",
      "ate\n",
      "100\n",
      "$\n",
      "of\n",
      "samosa\n",
      ".\n",
      "Then\n",
      "he\n",
      "said\n",
      "I\n",
      "can\n",
      "do\n",
      "this\n",
      "all\n",
      "day\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d6adee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cc7a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b2f8644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names\n",
    "\n",
    "# ner = named entity recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03eb19fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x292d7db9e20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x292d7923f40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x292d7b0ecf0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x292d804c540>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x292d7b1d280>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x292d780dc80>)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4cd78b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captain  |  PROPN  |  Captain\n",
      "america  |  PROPN  |  america\n",
      "ate  |  VERB  |  eat\n",
      "100  |  NUM  |  100\n",
      "$  |  NUM  |  $\n",
      "of  |  ADP  |  of\n",
      "samosa  |  PROPN  |  samosa\n",
      ".  |  PUNCT  |  .\n",
      "Then  |  ADV  |  then\n",
      "he  |  PRON  |  he\n",
      "said  |  VERB  |  say\n",
      "I  |  PRON  |  I\n",
      "can  |  AUX  |  can\n",
      "do  |  VERB  |  do\n",
      "this  |  PRON  |  this\n",
      "all  |  DET  |  all\n",
      "day  |  NOUN  |  day\n",
      ".  |  PUNCT  |  .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Captain america ate 100$ of samosa. Then he said I can do this all day.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \" | \", token.pos_,\" | \", token.lemma_)\n",
    "    \n",
    "# pos_ = part of speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff187d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"tesla Inc is going to acquire twitter for $45 billion\")\n",
    "# ent = entity\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57e92c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is going to acquire twitter for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $45 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d41d0691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fr-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.6.0/fr_core_news_sm-3.6.0-py3-none-any.whl (16.3 MB)\n",
      "                                              0.0/16.3 MB ? eta -:--:--\n",
      "                                              0.0/16.3 MB 1.3 MB/s eta 0:00:13\n",
      "                                             0.0/16.3 MB 487.6 kB/s eta 0:00:34\n",
      "                                             0.1/16.3 MB 819.2 kB/s eta 0:00:20\n",
      "                                             0.1/16.3 MB 717.5 kB/s eta 0:00:23\n",
      "                                             0.2/16.3 MB 748.1 kB/s eta 0:00:22\n",
      "                                              0.3/16.3 MB 1.0 MB/s eta 0:00:16\n",
      "     -                                        0.4/16.3 MB 1.3 MB/s eta 0:00:13\n",
      "     -                                        0.6/16.3 MB 1.6 MB/s eta 0:00:11\n",
      "     -                                        0.6/16.3 MB 1.5 MB/s eta 0:00:11\n",
      "     --                                       0.8/16.3 MB 1.7 MB/s eta 0:00:09\n",
      "     --                                       0.9/16.3 MB 1.9 MB/s eta 0:00:09\n",
      "     --                                       1.1/16.3 MB 1.9 MB/s eta 0:00:08\n",
      "     --                                       1.2/16.3 MB 2.0 MB/s eta 0:00:08\n",
      "     ---                                      1.4/16.3 MB 2.1 MB/s eta 0:00:08\n",
      "     ---                                      1.5/16.3 MB 2.1 MB/s eta 0:00:07\n",
      "     ----                                     1.6/16.3 MB 2.2 MB/s eta 0:00:07\n",
      "     ----                                     1.8/16.3 MB 2.3 MB/s eta 0:00:07\n",
      "     ----                                     1.9/16.3 MB 2.3 MB/s eta 0:00:07\n",
      "     -----                                    2.1/16.3 MB 2.3 MB/s eta 0:00:07\n",
      "     -----                                    2.2/16.3 MB 2.4 MB/s eta 0:00:06\n",
      "     -----                                    2.3/16.3 MB 2.4 MB/s eta 0:00:06\n",
      "     -----                                    2.3/16.3 MB 2.4 MB/s eta 0:00:06\n",
      "     ------                                   2.6/16.3 MB 2.5 MB/s eta 0:00:06\n",
      "     ------                                   2.8/16.3 MB 2.5 MB/s eta 0:00:06\n",
      "     -------                                  2.9/16.3 MB 2.5 MB/s eta 0:00:06\n",
      "     -------                                  3.1/16.3 MB 2.5 MB/s eta 0:00:06\n",
      "     -------                                  3.2/16.3 MB 2.6 MB/s eta 0:00:06\n",
      "     --------                                 3.4/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     --------                                 3.5/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     --------                                 3.6/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ---------                                3.7/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ---------                                3.8/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ---------                                3.9/16.3 MB 2.5 MB/s eta 0:00:05\n",
      "     ---------                                4.0/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ----------                               4.1/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ----------                               4.2/16.3 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------                               4.4/16.3 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------                               4.5/16.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -----------                              4.5/16.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -----------                              4.5/16.3 MB 2.5 MB/s eta 0:00:05\n",
      "     -----------                              4.9/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ------------                             5.0/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ------------                             5.1/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     ------------                             5.3/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     -------------                            5.4/16.3 MB 2.6 MB/s eta 0:00:05\n",
      "     --------------                           5.7/16.3 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------                           5.9/16.3 MB 2.7 MB/s eta 0:00:04\n",
      "     ---------------                          6.2/16.3 MB 2.8 MB/s eta 0:00:04\n",
      "     ---------------                          6.2/16.3 MB 2.8 MB/s eta 0:00:04\n",
      "     ---------------                          6.2/16.3 MB 2.7 MB/s eta 0:00:04\n",
      "     ---------------                          6.2/16.3 MB 2.7 MB/s eta 0:00:04\n",
      "     -----------------                        7.2/16.3 MB 3.0 MB/s eta 0:00:04\n",
      "     -----------------                        7.3/16.3 MB 3.0 MB/s eta 0:00:04\n",
      "     ------------------                       7.5/16.3 MB 3.0 MB/s eta 0:00:03\n",
      "     -------------------                      7.8/16.3 MB 3.1 MB/s eta 0:00:03\n",
      "     -------------------                      7.8/16.3 MB 3.1 MB/s eta 0:00:03\n",
      "     -------------------                      7.8/16.3 MB 3.1 MB/s eta 0:00:03\n",
      "     ---------------------                    8.9/16.3 MB 3.3 MB/s eta 0:00:03\n",
      "     ---------------------                    8.9/16.3 MB 3.3 MB/s eta 0:00:03\n",
      "     ----------------------                   9.0/16.3 MB 3.2 MB/s eta 0:00:03\n",
      "     -----------------------                  9.5/16.3 MB 3.3 MB/s eta 0:00:03\n",
      "     -----------------------                  9.7/16.3 MB 3.4 MB/s eta 0:00:02\n",
      "     ------------------------                 10.0/16.3 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------------------                10.2/16.3 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------------------                10.6/16.3 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------------               10.8/16.3 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------------------              11.2/16.3 MB 4.0 MB/s eta 0:00:02\n",
      "     ----------------------------             11.5/16.3 MB 4.1 MB/s eta 0:00:02\n",
      "     -----------------------------            11.8/16.3 MB 4.2 MB/s eta 0:00:02\n",
      "     -----------------------------            12.2/16.3 MB 4.3 MB/s eta 0:00:01\n",
      "     ------------------------------           12.6/16.3 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------          12.8/16.3 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------------         13.2/16.3 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------        13.7/16.3 MB 4.9 MB/s eta 0:00:01\n",
      "     ----------------------------------       14.0/16.3 MB 5.2 MB/s eta 0:00:01\n",
      "     -----------------------------------      14.3/16.3 MB 5.3 MB/s eta 0:00:01\n",
      "     -----------------------------------      14.6/16.3 MB 5.6 MB/s eta 0:00:01\n",
      "     ------------------------------------     15.0/16.3 MB 6.0 MB/s eta 0:00:01\n",
      "     -------------------------------------    15.3/16.3 MB 6.1 MB/s eta 0:00:01\n",
      "     --------------------------------------   15.6/16.3 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.9/16.3 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.9/16.3 MB 6.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  15.9/16.3 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  16.3/16.3 MB 6.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 16.3/16.3 MB 5.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from fr-core-news-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (67.7.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (4.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91755\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\91755\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-sm==3.6.0) (1.1.1)\n",
      "Installing collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab2c2f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "Twitter  |  MISC  |  Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "# ent = entity\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "938a9929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " va racheter \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twitter\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " pour $45 milliards de dollars</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style = \"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b796521",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "doc = nlp(\"Tesla Inc va racheter Twitter pour $45 milliards de dollars\")\n",
    "# ent = entity\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e395b688",
   "metadata": {},
   "source": [
    "Customizing blank pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ff0e5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "nlp.add_pipe(\"ner\", source = source_nlp)\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99f1750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla Inc  |  ORG  |  Companies, agencies, institutions, etc.\n",
      "$45 billion  |  MONEY  |  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
    "# ent = entity\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, \" | \", ent.label_, \" | \", spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dede2edc",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e54fbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aeeb7718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b42fddec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | ate\n",
      "adjustable | adjust\n",
      "rafting | raft\n",
      "ability | abil\n",
      "meeting | meet\n"
     ]
    }
   ],
   "source": [
    "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
    "\n",
    "for word in words:\n",
    "    print(word, \"|\", stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a85152e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat\n",
      "eats | eat\n",
      "eat | eat\n",
      "ate | eat\n",
      "adjustable | adjustable\n",
      "rafting | raft\n",
      "ability | ability\n",
      "meeting | meeting\n",
      "better | well\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa51cbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating | eat | 9837207709914848172\n",
      "eats | eat | 9837207709914848172\n",
      "eat | eat | 9837207709914848172\n",
      "ate | eat | 9837207709914848172\n",
      "adjustable | adjustable | 6033511944150694480\n",
      "rafting | raft | 7154368781129989833\n",
      "ability | ability | 11565809527369121409\n",
      "meeting | meeting | 14798207169164081740\n",
      "better | well | 4525988469032889948\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_, \"|\", token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "98a7b1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mando | Mando | 7837215228004622142\n",
      "talked | talk | 13939146775466599234\n",
      "for | for | 16037325823156266367\n",
      "3 | 3 | 602994839685422785\n",
      "hours | hour | 9748623380567160636\n",
      "although | although | 343236316598008647\n",
      "talking | talk | 13939146775466599234\n",
      "is | be | 10382539506755952630\n",
      "n't | not | 447765159362469301\n",
      "his | his | 2661093235354845946\n",
      "thing | thing | 2473243759842082748\n",
      "he | he | 1655312771067108281\n",
      "became | become | 12558846041070486771\n",
      "talkative | talkative | 13364764166055324990\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mando talked for 3 hours although talking isn't his thing he became talkative\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.lemma_, \"|\", token.lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1efd7ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7702c2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | bro\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brah\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fabb0b9",
   "metadata": {},
   "source": [
    "Customizing Attribute_ruler pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73e256c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro | Brother\n",
      ", | ,\n",
      "you | you\n",
      "wanna | wanna\n",
      "go | go\n",
      "? | ?\n",
      "Brah | Brother\n",
      ", | ,\n",
      "do | do\n",
      "n't | not\n",
      "say | say\n",
      "no | no\n",
      "! | !\n",
      "I | I\n",
      "am | be\n",
      "exhausted | exhaust\n"
     ]
    }
   ],
   "source": [
    "ar = nlp.get_pipe('attribute_ruler')\n",
    "\n",
    "ar.add([[{\"TEXT\": \"Bro\"}], [{\"TEXT\": \"Brah\"}]], {\"LEMMA\": \"Brother\"})\n",
    "\n",
    "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"|\", token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cb618d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec7feaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon | PROPN | 96 | proper noun\n",
      "flew | VERB | 100 | verb\n",
      "to | ADP | 85 | adposition\n",
      "mars | NOUN | 92 | noun\n",
      "yesterday | NOUN | 92 | noun\n",
      ". | PUNCT | 97 | punctuation\n",
      "He | PRON | 95 | pronoun\n",
      "carried | VERB | 100 | verb\n",
      "biryani | ADJ | 84 | adjective\n",
      "masala | NOUN | 92 | noun\n",
      "with | ADP | 85 | adposition\n",
      "him | PRON | 95 | pronoun\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Elon flew to mars yesterday. He carried biryani masala with him\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", token.pos_, \"|\", token.pos, \"|\", spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379fb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADJ (adjective) = 84<hr>\n",
    "ADP (adposition(conatins prepositions & postpositions)) = 85<hr>\n",
    "ADV (adverb) = 86<hr>\n",
    "AUX (auxiliary) = 87<hr>\n",
    "CCONJ (coordinating conjunction) = 89<hr>\n",
    "DET (determiner) = 90<hr>\n",
    "INTJ (interjection) = 91<hr>\n",
    "NOUN (noun) = 92<hr>\n",
    "NUM (numeral) = 93<hr>\n",
    "PART (particle) = 94<hr>\n",
    "PRON (pronoun) = 95<hr>\n",
    "PROPN (proper noun) = 96<hr>\n",
    "PUNCT (punctuation) = 97<hr>\n",
    "SCONJ (subordinating conjunction) = 98<hr>\n",
    "SYM (symbol) = 99<hr>\n",
    "VERB (verb) = 100<hr>\n",
    "X (other) = 101<hr>\n",
    "SPACE (space) = 103<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "26e0fae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wow | INTJ | 91 | interjection | UH | interjection\n",
      "! | PUNCT | 97 | punctuation | . | punctuation mark, sentence closer\n",
      "Dr. | PROPN | 96 | proper noun | NNP | noun, proper singular\n",
      "Strange | PROPN | 96 | proper noun | NNP | noun, proper singular\n",
      "made | VERB | 100 | verb | VBD | verb, past tense\n",
      "265 | NUM | 93 | numeral | CD | cardinal number\n",
      "million | NUM | 93 | numeral | CD | cardinal number\n",
      "$ | NUM | 93 | numeral | CD | cardinal number\n",
      "on | ADP | 85 | adposition | IN | conjunction, subordinating or preposition\n",
      "the | DET | 90 | determiner | DT | determiner\n",
      "very | ADV | 86 | adverb | RB | adverb\n",
      "first | ADJ | 84 | adjective | JJ | adjective (English), other noun-modifier (Chinese)\n",
      "day | NOUN | 92 | noun | NN | noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wow! Dr. Strange made 265 million $ on the very first day\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\", \n",
    "          token.pos_, \"|\", \n",
    "          token.pos, \"|\", \n",
    "          spacy.explain(token.pos_), \"|\", \n",
    "          token.tag_, \"|\", \n",
    "          spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f9eaef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quits | VBZ | verb, 3rd person singular present\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"He quits the job\")\n",
    "doc[1]\n",
    "\n",
    "print(doc[1].text, \"|\",\n",
    "     doc[1].tag_, \"|\",\n",
    "     spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a2d5b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quit | VBD | verb, past tense\n"
     ]
    }
   ],
   "source": [
    "doc= nlp(\"He quit the job\")\n",
    "doc[1]\n",
    "\n",
    "print(doc[1].text, \"|\",\n",
    "     doc[1].tag_, \"|\",\n",
    "     spacy.explain(doc[1].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beda006",
   "metadata": {},
   "source": [
    "hands-on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "73812d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "earning_text = '''Microsoft Corp. today announced the following results for the quarter ended June 30, 2023, as compared to the corresponding period of last fiscal year:\n",
    "\n",
    "·        Revenue was $56.2 billion and increased 8% (up 10% in constant currency)\n",
    "\n",
    "·        Operating income was $24.3 billion and increased 18% (up 21% in constant currency)\n",
    "\n",
    "·        Net income was $20.1 billion and increased 20% (up 23% in constant currency)\n",
    "\n",
    "·        Diluted earnings per share was $2.69 and increased 21% (up 23% in constant currency)\n",
    "\n",
    "“Organizations are etc. asking not only how – but how fast – they can apply this next generation of AI to address the biggest opportunities and challenges they face – safely and responsibly,” said Satya Nadella, chairman and chief executive officer of Microsoft. \n",
    "“We remain focused on leading the new AI platform shift, helping customers use the Microsoft Cloud to get the most value out of their digital spend, and driving operating leverage.”'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a100fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "440cd3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft | PROPN | 96 | proper noun\n",
      "Corp. | PROPN | 96 | proper noun\n",
      "today | NOUN | 92 | noun\n",
      "announced | VERB | 100 | verb\n",
      "the | DET | 90 | determiner\n",
      "following | VERB | 100 | verb\n",
      "results | NOUN | 92 | noun\n",
      "for | ADP | 85 | adposition\n",
      "the | DET | 90 | determiner\n",
      "quarter | NOUN | 92 | noun\n",
      "ended | VERB | 100 | verb\n",
      "June | PROPN | 96 | proper noun\n",
      "30 | NUM | 93 | numeral\n",
      ", | PUNCT | 97 | punctuation\n",
      "2023 | NUM | 93 | numeral\n",
      ", | PUNCT | 97 | punctuation\n",
      "as | SCONJ | 98 | subordinating conjunction\n",
      "compared | VERB | 100 | verb\n",
      "to | ADP | 85 | adposition\n",
      "the | DET | 90 | determiner\n",
      "corresponding | ADJ | 84 | adjective\n",
      "period | NOUN | 92 | noun\n",
      "of | ADP | 85 | adposition\n",
      "last | ADJ | 84 | adjective\n",
      "fiscal | ADJ | 84 | adjective\n",
      "year | NOUN | 92 | noun\n",
      ": | PUNCT | 97 | punctuation\n",
      "\n",
      "\n",
      " | SPACE | 103 | space\n",
      "· | PUNCT | 97 | punctuation\n",
      "        | SPACE | 103 | space\n",
      "Revenue | NOUN | 92 | noun\n",
      "was | AUX | 87 | auxiliary\n",
      "$ | SYM | 99 | symbol\n",
      "56.2 | NUM | 93 | numeral\n",
      "billion | NUM | 93 | numeral\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "increased | VERB | 100 | verb\n",
      "8 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "( | PUNCT | 97 | punctuation\n",
      "up | ADV | 86 | adverb\n",
      "10 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "in | ADP | 85 | adposition\n",
      "constant | ADJ | 84 | adjective\n",
      "currency | NOUN | 92 | noun\n",
      ") | PUNCT | 97 | punctuation\n",
      "\n",
      "\n",
      " | SPACE | 103 | space\n",
      "· | PUNCT | 97 | punctuation\n",
      "        | SPACE | 103 | space\n",
      "Operating | VERB | 100 | verb\n",
      "income | NOUN | 92 | noun\n",
      "was | AUX | 87 | auxiliary\n",
      "$ | SYM | 99 | symbol\n",
      "24.3 | NUM | 93 | numeral\n",
      "billion | NUM | 93 | numeral\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "increased | VERB | 100 | verb\n",
      "18 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "( | PUNCT | 97 | punctuation\n",
      "up | ADV | 86 | adverb\n",
      "21 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "in | ADP | 85 | adposition\n",
      "constant | ADJ | 84 | adjective\n",
      "currency | NOUN | 92 | noun\n",
      ") | PUNCT | 97 | punctuation\n",
      "\n",
      "\n",
      " | SPACE | 103 | space\n",
      "· | PUNCT | 97 | punctuation\n",
      "        | SPACE | 103 | space\n",
      "Net | ADJ | 84 | adjective\n",
      "income | NOUN | 92 | noun\n",
      "was | AUX | 87 | auxiliary\n",
      "$ | SYM | 99 | symbol\n",
      "20.1 | NUM | 93 | numeral\n",
      "billion | NUM | 93 | numeral\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "increased | VERB | 100 | verb\n",
      "20 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "( | PUNCT | 97 | punctuation\n",
      "up | ADV | 86 | adverb\n",
      "23 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "in | ADP | 85 | adposition\n",
      "constant | ADJ | 84 | adjective\n",
      "currency | NOUN | 92 | noun\n",
      ") | PUNCT | 97 | punctuation\n",
      "\n",
      "\n",
      " | SPACE | 103 | space\n",
      "· | PUNCT | 97 | punctuation\n",
      "        | SPACE | 103 | space\n",
      "Diluted | VERB | 100 | verb\n",
      "earnings | NOUN | 92 | noun\n",
      "per | ADP | 85 | adposition\n",
      "share | NOUN | 92 | noun\n",
      "was | AUX | 87 | auxiliary\n",
      "$ | SYM | 99 | symbol\n",
      "2.69 | NUM | 93 | numeral\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "increased | VERB | 100 | verb\n",
      "21 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "( | PUNCT | 97 | punctuation\n",
      "up | ADV | 86 | adverb\n",
      "23 | NUM | 93 | numeral\n",
      "% | NOUN | 92 | noun\n",
      "in | ADP | 85 | adposition\n",
      "constant | ADJ | 84 | adjective\n",
      "currency | NOUN | 92 | noun\n",
      ") | PUNCT | 97 | punctuation\n",
      "\n",
      "\n",
      " | SPACE | 103 | space\n",
      "“ | PUNCT | 97 | punctuation\n",
      "Organizations | NOUN | 92 | noun\n",
      "are | AUX | 87 | auxiliary\n",
      "etc | X | 101 | other\n",
      ". | X | 101 | other\n",
      "asking | VERB | 100 | verb\n",
      "not | PART | 94 | particle\n",
      "only | ADV | 86 | adverb\n",
      "how | SCONJ | 98 | subordinating conjunction\n",
      "– | PUNCT | 97 | punctuation\n",
      "but | CCONJ | 89 | coordinating conjunction\n",
      "how | SCONJ | 98 | subordinating conjunction\n",
      "fast | ADV | 86 | adverb\n",
      "– | PUNCT | 97 | punctuation\n",
      "they | PRON | 95 | pronoun\n",
      "can | AUX | 87 | auxiliary\n",
      "apply | VERB | 100 | verb\n",
      "this | DET | 90 | determiner\n",
      "next | ADJ | 84 | adjective\n",
      "generation | NOUN | 92 | noun\n",
      "of | ADP | 85 | adposition\n",
      "AI | PROPN | 96 | proper noun\n",
      "to | PART | 94 | particle\n",
      "address | VERB | 100 | verb\n",
      "the | DET | 90 | determiner\n",
      "biggest | ADJ | 84 | adjective\n",
      "opportunities | NOUN | 92 | noun\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "challenges | NOUN | 92 | noun\n",
      "they | PRON | 95 | pronoun\n",
      "face | VERB | 100 | verb\n",
      "– | PUNCT | 97 | punctuation\n",
      "safely | ADV | 86 | adverb\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "responsibly | ADV | 86 | adverb\n",
      ", | PUNCT | 97 | punctuation\n",
      "” | PUNCT | 97 | punctuation\n",
      "said | VERB | 100 | verb\n",
      "Satya | PROPN | 96 | proper noun\n",
      "Nadella | PROPN | 96 | proper noun\n",
      ", | PUNCT | 97 | punctuation\n",
      "chairman | NOUN | 92 | noun\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "chief | ADJ | 84 | adjective\n",
      "executive | ADJ | 84 | adjective\n",
      "officer | NOUN | 92 | noun\n",
      "of | ADP | 85 | adposition\n",
      "Microsoft | PROPN | 96 | proper noun\n",
      ". | PUNCT | 97 | punctuation\n",
      "\n",
      " | SPACE | 103 | space\n",
      "“ | PUNCT | 97 | punctuation\n",
      "We | PRON | 95 | pronoun\n",
      "remain | VERB | 100 | verb\n",
      "focused | ADJ | 84 | adjective\n",
      "on | ADP | 85 | adposition\n",
      "leading | VERB | 100 | verb\n",
      "the | DET | 90 | determiner\n",
      "new | ADJ | 84 | adjective\n",
      "AI | PROPN | 96 | proper noun\n",
      "platform | NOUN | 92 | noun\n",
      "shift | NOUN | 92 | noun\n",
      ", | PUNCT | 97 | punctuation\n",
      "helping | VERB | 100 | verb\n",
      "customers | NOUN | 92 | noun\n",
      "use | VERB | 100 | verb\n",
      "the | DET | 90 | determiner\n",
      "Microsoft | PROPN | 96 | proper noun\n",
      "Cloud | PROPN | 96 | proper noun\n",
      "to | PART | 94 | particle\n",
      "get | VERB | 100 | verb\n",
      "the | DET | 90 | determiner\n",
      "most | ADJ | 84 | adjective\n",
      "value | NOUN | 92 | noun\n",
      "out | ADP | 85 | adposition\n",
      "of | ADP | 85 | adposition\n",
      "their | PRON | 95 | pronoun\n",
      "digital | ADJ | 84 | adjective\n",
      "spend | NOUN | 92 | noun\n",
      ", | PUNCT | 97 | punctuation\n",
      "and | CCONJ | 89 | coordinating conjunction\n",
      "driving | VERB | 100 | verb\n",
      "operating | NOUN | 92 | noun\n",
      "leverage | NOUN | 92 | noun\n",
      ". | PUNCT | 97 | punctuation\n",
      "” | PUNCT | 97 | punctuation\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(earning_text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, \"|\",\n",
    "         token.pos_, \"|\",\n",
    "         token.pos, \"|\",\n",
    "         spacy.explain(token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_spans = list(doc.sents)\n",
    "displacy.serve(sentence_spans, style=\"dep\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
